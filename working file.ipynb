{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Firstly we load all of our fundamental data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c9799c4dea341fe1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Requirements:\n",
    "Download all the modules given in the requirements.txt file\n",
    "Download the following models\n",
    "(https://huggingface.co/nomic-ai/nomic-embed-text-v1 - embedding model)\n",
    "(https://huggingface.co/mixedbread-ai/mxbai-rerank-large-v1 - ReRanker mode)\n",
    "The Large language model (mistral 7b it) will be downloaded as the code is run\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b69f06684d977d85"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-13T13:15:51.980622200Z",
     "start_time": "2024-04-13T13:15:51.517197500Z"
    }
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Failed to import transformers.pipelines because of the following error (look up to see its traceback):\nmodule 'torch' has no attribute 'version'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "File \u001B[1;32m~\\Documents\\python\\GDSC\\RAG\\lib\\site-packages\\transformers\\utils\\import_utils.py:1390\u001B[0m, in \u001B[0;36m_LazyModule._get_module\u001B[1;34m(self, module_name)\u001B[0m\n\u001B[0;32m   1389\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 1390\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mimportlib\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mimport_module\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m.\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mmodule_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;18;43m__name__\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1391\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\importlib\\__init__.py:126\u001B[0m, in \u001B[0;36mimport_module\u001B[1;34m(name, package)\u001B[0m\n\u001B[0;32m    125\u001B[0m         level \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m--> 126\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_bootstrap\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_gcd_import\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m[\u001B[49m\u001B[43mlevel\u001B[49m\u001B[43m:\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpackage\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlevel\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap>:1050\u001B[0m, in \u001B[0;36m_gcd_import\u001B[1;34m(name, package, level)\u001B[0m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap>:1027\u001B[0m, in \u001B[0;36m_find_and_load\u001B[1;34m(name, import_)\u001B[0m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap>:1006\u001B[0m, in \u001B[0;36m_find_and_load_unlocked\u001B[1;34m(name, import_)\u001B[0m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap>:688\u001B[0m, in \u001B[0;36m_load_unlocked\u001B[1;34m(spec)\u001B[0m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap_external>:883\u001B[0m, in \u001B[0;36mexec_module\u001B[1;34m(self, module)\u001B[0m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap>:241\u001B[0m, in \u001B[0;36m_call_with_frames_removed\u001B[1;34m(f, *args, **kwds)\u001B[0m\n",
      "File \u001B[1;32m~\\Documents\\python\\GDSC\\RAG\\lib\\site-packages\\transformers\\pipelines\\__init__.py:47\u001B[0m\n\u001B[0;32m     33\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m     34\u001B[0m     CONFIG_NAME,\n\u001B[0;32m     35\u001B[0m     HUGGINGFACE_CO_RESOLVE_ENDPOINT,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     45\u001B[0m     logging,\n\u001B[0;32m     46\u001B[0m )\n\u001B[1;32m---> 47\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01maudio_classification\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m AudioClassificationPipeline\n\u001B[0;32m     48\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mautomatic_speech_recognition\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m AutomaticSpeechRecognitionPipeline\n",
      "File \u001B[1;32m~\\Documents\\python\\GDSC\\RAG\\lib\\site-packages\\transformers\\pipelines\\audio_classification.py:21\u001B[0m\n\u001B[0;32m     20\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m add_end_docstrings, is_torch_available, is_torchaudio_available, logging\n\u001B[1;32m---> 21\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbase\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Pipeline, build_pipeline_init_args\n\u001B[0;32m     24\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_torch_available():\n",
      "File \u001B[1;32m~\\Documents\\python\\GDSC\\RAG\\lib\\site-packages\\transformers\\pipelines\\base.py:34\u001B[0m\n\u001B[0;32m     33\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mimage_processing_utils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m BaseImageProcessor\n\u001B[1;32m---> 34\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodelcard\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ModelCard\n\u001B[0;32m     35\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodels\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mauto\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mconfiguration_auto\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m AutoConfig\n",
      "File \u001B[1;32m~\\Documents\\python\\GDSC\\RAG\\lib\\site-packages\\transformers\\modelcard.py:48\u001B[0m\n\u001B[0;32m     32\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodels\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mauto\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodeling_auto\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m     33\u001B[0m     MODEL_FOR_AUDIO_CLASSIFICATION_MAPPING_NAMES,\n\u001B[0;32m     34\u001B[0m     MODEL_FOR_CAUSAL_LM_MAPPING_NAMES,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     46\u001B[0m     MODEL_FOR_ZERO_SHOT_IMAGE_CLASSIFICATION_MAPPING_NAMES,\n\u001B[0;32m     47\u001B[0m )\n\u001B[1;32m---> 48\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtraining_args\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ParallelMode\n\u001B[0;32m     49\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m     50\u001B[0m     MODEL_CARD_NAME,\n\u001B[0;32m     51\u001B[0m     cached_file,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     57\u001B[0m     logging,\n\u001B[0;32m     58\u001B[0m )\n",
      "File \u001B[1;32m~\\Documents\\python\\GDSC\\RAG\\lib\\site-packages\\transformers\\training_args.py:71\u001B[0m\n\u001B[0;32m     70\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01maccelerate\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mstate\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m AcceleratorState, PartialState\n\u001B[1;32m---> 71\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01maccelerate\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m DistributedType\n\u001B[0;32m     73\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtrainer_pt_utils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m AcceleratorConfig\n",
      "File \u001B[1;32m~\\Documents\\python\\GDSC\\RAG\\lib\\site-packages\\accelerate\\__init__.py:16\u001B[0m\n\u001B[0;32m     14\u001B[0m __version__ \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m0.29.1\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m---> 16\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01maccelerator\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Accelerator\n\u001B[0;32m     17\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbig_modeling\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m     18\u001B[0m     cpu_offload,\n\u001B[0;32m     19\u001B[0m     cpu_offload_with_hook,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     24\u001B[0m     load_checkpoint_and_dispatch,\n\u001B[0;32m     25\u001B[0m )\n",
      "File \u001B[1;32m~\\Documents\\python\\GDSC\\RAG\\lib\\site-packages\\accelerate\\accelerator.py:35\u001B[0m\n\u001B[0;32m     33\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mhooks\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mhooks\u001B[39;00m\n\u001B[1;32m---> 35\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcheckpointing\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m load_accelerator_state, load_custom_state, save_accelerator_state, save_custom_state\n\u001B[0;32m     36\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata_loader\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m DataLoaderDispatcher, prepare_data_loader, skip_first_batches\n",
      "File \u001B[1;32m~\\Documents\\python\\GDSC\\RAG\\lib\\site-packages\\accelerate\\checkpointing.py:24\u001B[0m\n\u001B[0;32m     22\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcuda\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mamp\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m GradScaler\n\u001B[1;32m---> 24\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m     25\u001B[0m     MODEL_NAME,\n\u001B[0;32m     26\u001B[0m     OPTIMIZER_NAME,\n\u001B[0;32m     27\u001B[0m     RNG_STATE_NAME,\n\u001B[0;32m     28\u001B[0m     SAFE_MODEL_NAME,\n\u001B[0;32m     29\u001B[0m     SAFE_WEIGHTS_NAME,\n\u001B[0;32m     30\u001B[0m     SAMPLER_NAME,\n\u001B[0;32m     31\u001B[0m     SCALER_NAME,\n\u001B[0;32m     32\u001B[0m     SCHEDULER_NAME,\n\u001B[0;32m     33\u001B[0m     WEIGHTS_NAME,\n\u001B[0;32m     34\u001B[0m     get_pretty_name,\n\u001B[0;32m     35\u001B[0m     is_torch_xla_available,\n\u001B[0;32m     36\u001B[0m     is_xpu_available,\n\u001B[0;32m     37\u001B[0m     save,\n\u001B[0;32m     38\u001B[0m )\n\u001B[0;32m     41\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_torch_xla_available():\n",
      "File \u001B[1;32m~\\Documents\\python\\GDSC\\RAG\\lib\\site-packages\\accelerate\\utils\\__init__.py:178\u001B[0m\n\u001B[0;32m    177\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbnb\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m has_4bit_bnb_layers, load_and_quantize_model\n\u001B[1;32m--> 178\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfsdp_utils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m load_fsdp_model, load_fsdp_optimizer, save_fsdp_model, save_fsdp_optimizer\n\u001B[0;32m    179\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlaunch\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m    180\u001B[0m     PrepareForLaunch,\n\u001B[0;32m    181\u001B[0m     _filter_args,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    186\u001B[0m     prepare_tpu,\n\u001B[0;32m    187\u001B[0m )\n",
      "File \u001B[1;32m~\\Documents\\python\\GDSC\\RAG\\lib\\site-packages\\accelerate\\utils\\fsdp_utils.py:26\u001B[0m\n\u001B[0;32m     25\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_torch_version(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m>=\u001B[39m\u001B[38;5;124m\"\u001B[39m, FSDP_PYTORCH_VERSION) \u001B[38;5;129;01mand\u001B[39;00m is_torch_distributed_available():\n\u001B[1;32m---> 26\u001B[0m     \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdistributed\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcheckpoint\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mdist_cp\u001B[39;00m\n\u001B[0;32m     27\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdistributed\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcheckpoint\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdefault_planner\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m DefaultLoadPlanner, DefaultSavePlanner\n",
      "File \u001B[1;32m~\\Documents\\python\\GDSC\\RAG\\lib\\site-packages\\torch\\distributed\\checkpoint\\__init__.py:7\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmetadata\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m      2\u001B[0m     TensorStorageMetadata,\n\u001B[0;32m      3\u001B[0m     BytesStorageMetadata,\n\u001B[0;32m      4\u001B[0m     ChunkStorageMetadata,\n\u001B[0;32m      5\u001B[0m     Metadata,\n\u001B[0;32m      6\u001B[0m )\n\u001B[1;32m----> 7\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mstate_dict_loader\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m load_state_dict, load\n\u001B[0;32m      8\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mstate_dict_saver\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m save_state_dict, save\n",
      "File \u001B[1;32m~\\Documents\\python\\GDSC\\RAG\\lib\\site-packages\\torch\\distributed\\checkpoint\\state_dict_loader.py:12\u001B[0m\n\u001B[0;32m     11\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mplanner\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m LoadPlanner\n\u001B[1;32m---> 12\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdefault_planner\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m DefaultLoadPlanner\n\u001B[0;32m     14\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m _DistWrapper, _all_gather_keys\n",
      "File \u001B[1;32m~\\Documents\\python\\GDSC\\RAG\\lib\\site-packages\\torch\\distributed\\checkpoint\\default_planner.py:14\u001B[0m\n\u001B[0;32m     13\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdistributed\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_shard\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_utils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m narrow_tensor_by_index\n\u001B[1;32m---> 14\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdistributed\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_tensor\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m DTensor\n\u001B[0;32m     17\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdistributed\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcheckpoint\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mplanner\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m     18\u001B[0m     SavePlanner,\n\u001B[0;32m     19\u001B[0m     LoadPlanner,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     24\u001B[0m     WriteItemType,\n\u001B[0;32m     25\u001B[0m )\n",
      "File \u001B[1;32m~\\Documents\\python\\GDSC\\RAG\\lib\\site-packages\\torch\\distributed\\_tensor\\__init__.py:346\u001B[0m\n\u001B[0;32m    345\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_running_with_deploy():\n\u001B[1;32m--> 346\u001B[0m     \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdistributed\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_tensor\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_dynamo_utils\u001B[39;00m\n",
      "File \u001B[1;32m~\\Documents\\python\\GDSC\\RAG\\lib\\site-packages\\torch\\distributed\\_tensor\\_dynamo_utils.py:1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_dynamo\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m allow_in_graph\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdistributed\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_tensor\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mapi\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m DTensor\n",
      "File \u001B[1;32m~\\Documents\\python\\GDSC\\RAG\\lib\\site-packages\\torch\\_dynamo\\__init__.py:2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m allowed_functions, convert_frame, eval_frame, resume_execution\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbackends\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mregistry\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m list_backends, lookup_backend, register_backend\n",
      "File \u001B[1;32m~\\Documents\\python\\GDSC\\RAG\\lib\\site-packages\\torch\\_dynamo\\allowed_functions.py:28\u001B[0m\n\u001B[0;32m     26\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfx\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_symbolic_trace\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m is_fx_tracing\n\u001B[1;32m---> 28\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m config\n\u001B[0;32m     29\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mexternal_utils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m is_compiling\n",
      "File \u001B[1;32m~\\Documents\\python\\GDSC\\RAG\\lib\\site-packages\\torch\\_dynamo\\config.py:288\u001B[0m\n\u001B[0;32m    285\u001B[0m     debug_dir_root \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(  \u001B[38;5;66;03m# [@compile_ignored: debug]\u001B[39;00m\n\u001B[0;32m    286\u001B[0m         os\u001B[38;5;241m.\u001B[39menviron[DEBUG_DIR_VAR_NAME], \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtorch_compile_debug\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    287\u001B[0m     )\n\u001B[1;32m--> 288\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[43mis_fbcode\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[0;32m    289\u001B[0m     debug_dir_root \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(  \u001B[38;5;66;03m# [@compile_ignored: debug]\u001B[39;00m\n\u001B[0;32m    290\u001B[0m         tempfile\u001B[38;5;241m.\u001B[39mgettempdir(), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtorch_compile_debug\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    291\u001B[0m     )\n",
      "File \u001B[1;32m~\\Documents\\python\\GDSC\\RAG\\lib\\site-packages\\torch\\_dynamo\\config.py:279\u001B[0m, in \u001B[0;36mis_fbcode\u001B[1;34m()\u001B[0m\n\u001B[0;32m    278\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mis_fbcode\u001B[39m():\n\u001B[1;32m--> 279\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mversion\u001B[49m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgit_version\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\Documents\\python\\GDSC\\RAG\\lib\\site-packages\\torch\\__init__.py:1938\u001B[0m, in \u001B[0;36m__getattr__\u001B[1;34m(name)\u001B[0m\n\u001B[0;32m   1936\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m importlib\u001B[38;5;241m.\u001B[39mimport_module(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;18m__name__\u001B[39m)\n\u001B[1;32m-> 1938\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAttributeError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodule \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m has no attribute \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[1;31mAttributeError\u001B[0m: module 'torch' has no attribute 'version'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[3], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mos\u001B[39;00m\n\u001B[1;32m----> 3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtransformers\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtransformers\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m is_flash_attn_2_available\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnp\u001B[39;00m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap>:1075\u001B[0m, in \u001B[0;36m_handle_fromlist\u001B[1;34m(module, fromlist, import_, recursive)\u001B[0m\n",
      "File \u001B[1;32m~\\Documents\\python\\GDSC\\RAG\\lib\\site-packages\\transformers\\utils\\import_utils.py:1380\u001B[0m, in \u001B[0;36m_LazyModule.__getattr__\u001B[1;34m(self, name)\u001B[0m\n\u001B[0;32m   1378\u001B[0m     value \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_module(name)\n\u001B[0;32m   1379\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m name \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_class_to_module\u001B[38;5;241m.\u001B[39mkeys():\n\u001B[1;32m-> 1380\u001B[0m     module \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_module\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_class_to_module\u001B[49m\u001B[43m[\u001B[49m\u001B[43mname\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1381\u001B[0m     value \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(module, name)\n\u001B[0;32m   1382\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[1;32m~\\Documents\\python\\GDSC\\RAG\\lib\\site-packages\\transformers\\utils\\import_utils.py:1392\u001B[0m, in \u001B[0;36m_LazyModule._get_module\u001B[1;34m(self, module_name)\u001B[0m\n\u001B[0;32m   1390\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m importlib\u001B[38;5;241m.\u001B[39mimport_module(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m module_name, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m)\n\u001B[0;32m   1391\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m-> 1392\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[0;32m   1393\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFailed to import \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmodule_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m because of the following error (look up to see its\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1394\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m traceback):\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00me\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1395\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: Failed to import transformers.pipelines because of the following error (look up to see its traceback):\nmodule 'torch' has no attribute 'version'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
    "from transformers.utils import is_flash_attn_2_available\n",
    "import numpy as np\n",
    "from daft import DataFrame\n",
    "from tqdm.auto import tqdm\n",
    "import typing\n",
    "import pandas as pd\n",
    "from spacy.lang.en import English\n",
    "import time\n",
    "import re\n",
    "from sentence_transformers import SentenceTransformer, util, CrossEncoder\n",
    "import textwrap\n",
    "k= 15\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "#Loading Data from csv file\n",
    "text_chunks_and_embeddings_df = pd.read_csv('text_chunks_and_embeddings_df.csv')\n",
    "\n",
    "#convert to a numpy array from a string (as we are reading it from a csv file)\n",
    "text_chunks_and_embeddings_df['embedding'] = text_chunks_and_embeddings_df['embedding'].apply(lambda x: \n",
    "                           np.fromstring(\n",
    "                               x.strip('[]'), sep = ' '))\n",
    "embeddings = torch.tensor(np.stack(text_chunks_and_embeddings_df['embedding'].tolist(), axis = 0), dtype=torch.float32).to(device)\n",
    "                                \n",
    "\n",
    "scripts_with_chunks = text_chunks_and_embeddings_df.to_dict(orient = 'records')\n",
    "\n",
    "embedding_model = SentenceTransformer('nomic-embed-text-v1', trust_remote_code=True, device = device)\n",
    "\n",
    "ReRanker = CrossEncoder(\"mixedbread-aimxbai-rerank-large-v1\",device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "torch and np return diff data types (f32 and f64) so make sure to address that before dot product\n",
    "\n",
    "Semantic search Steps\n",
    "We have to return all strings that are relevant to a given query (using the embeddings we have generated\n",
    "1)Define a queryy string\n",
    "2)Turn the query string into an embedding\n",
    "3)Perform a dot product of cosine similarity function between the text embeddings and the query embedding\n",
    "4)Sort the results from 3 in ascending order"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cdd3a01021f4b9b1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Searching over embeddings is insanely fast, you might want to use an index when you cross well over 10,000-100,000 times the amount of data we are dealing with right now\n",
    "\n",
    "Now we make the printed data presentable"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1ea4c9ea3533d547"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def print_wrapped(text, wrap_length = 80):\n",
    "    wrap_text = textwrap.fill(text, wrap_length)\n",
    "    print(wrap_text)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-13T13:15:51.981621200Z",
     "start_time": "2024-04-13T13:15:51.981621200Z"
    }
   },
   "id": "c14f79e62ef6cd8d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can re rank the results we recieve from this and make it into functions"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7f32366def3bb808"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def retrieve_relevant_resources(query:str,\n",
    "                                embeddings:torch.tensor,\n",
    "                                n_resources_to_return = k):\n",
    "    #Query Embedding\n",
    "    query_embedding = embedding_model.encode(query, convert_to_tensor=True).to(device)\n",
    "      \n",
    "        \n",
    "    #Dot product\n",
    "    dot_scores= util.dot_score(query_embedding, embeddings)[0]\n",
    "    scores, indices = torch.topk(dot_scores, k=n_resources_to_return)\n",
    "    \n",
    "    return scores, indices\n",
    "\n",
    "def print_top_results_and_scores(query:str,\n",
    "                                 embeddings: torch.tensor,\n",
    "                                 scripts_with_chunks: list[dict] = scripts_with_chunks,\n",
    "                                 n_resources_to_return = k):\n",
    "    \n",
    "    \n",
    "    scores, indices = retrieve_relevant_resources(query=query, embeddings=embeddings, n_resources_to_return=n_resources_to_return)\n",
    "    documents=[]\n",
    "    for index in indices:\n",
    "        Text = scripts_with_chunks[index]['chunks']\n",
    "        documents.append(Text)\n",
    "        \n",
    "        \n",
    "    results = ReRanker.rank(query, documents, return_documents=True, top_k = k) #Returns [{corpus id, Score, text}]\n",
    "    print(\"Results\")\n",
    "    for corpus in results: \n",
    "        corpus_id = corpus['corpus_id']\n",
    "        Score = corpus['score']\n",
    "        text = corpus['text']\n",
    "        print(f'Score: {Score:.4f}')\n",
    "        print(f'Text: {text}')\n",
    "        print(f\"title: {scripts_with_chunks[indices[corpus_id]]['title']}\")\n",
    "    \n",
    "#print_top_results_and_scores(query=query, embeddings=embeddings,  n_resources_to_return=k)\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-13T13:15:51.984621900Z"
    }
   },
   "id": "5bee45105196ec14"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Local LLM implementation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e883edbcb7528e55"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#quantization configuration\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "#checks if flash_attn_2 is available\n",
    "if (is_flash_attn_2_available()) and (torch.cuda.get_device_capability()[0]>=8):\n",
    "    attn_implementation = 'flash_attention_2'\n",
    "else:\n",
    "    attn_implementation = 'sdpa'   #scaled dot product attention\n",
    "\n",
    "attn_implementation\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=bnb_config,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "        attn_implementation =  attn_implementation,    \n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-13T13:15:51.986621600Z"
    }
   },
   "id": "2c7b4cac66c64175"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_model_num_params(model:torch.nn.Module):\n",
    "    return sum([param.numel() for param in model.parameters()])\n",
    "get_model_num_params(model)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-13T13:15:51.987622800Z"
    }
   },
   "id": "ea194b6927346192"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Generate text with our LLM"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ec86340afbcaee01"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Creation of a Prompt Template"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4275deed6c3ae563"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Set of Instructions that the LLM should follow\n",
    "instruction_str = '''1)The given Query relates to a question about movies from one of the following\n",
    "Avengers, The How to Train Your Dragon Trilogy and The Titanic\n",
    "2) The returned answer must be the answer to the question\n",
    "3)If more info must be required to answer the question, reply with a request for more context to the question, so that you can provide a good answer'''\n",
    "\n",
    "input_text = 'read out a poem to me about how to commit tax evasion'\n",
    "dialogue_template = [{\n",
    "    'instruction': instruction_str,\n",
    "    'role':'user',\n",
    "    'content':input_text\n",
    "}]\n",
    "\n",
    "\n",
    "#Applying thr prompt template\n",
    "prompt = tokenizer.apply_chat_template(conversation=dialogue_template, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "print(prompt)\n",
    "input_ids = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "\n",
    "#Generate outputs from local llm\n",
    "output = model.generate(**input_ids, max_new_tokens = 1000)\n",
    "\n",
    "#decode output\n",
    "output_text = tokenizer.decode(output[0])\n",
    "print(f'Model Output: {output_text}')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-13T13:15:51.989620900Z"
    }
   },
   "id": "7656d0927ab34bad"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Augmenting our prompt with necessary context\n",
    "After Retrival and Generation, we gonna add augmentation\n",
    "\n",
    "It could be considered a form of prompt engineering, so we must\n",
    "1)Give very clear instructions\n",
    "2)Give a few examples of input/output\n",
    "3)Give it room to think\n",
    "\n",
    "So we will implement a function to do so"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2ecee33ea8e609c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#prompt formatting\n",
    "def prompt_formatter(query:str, context_items:list[dict]) -> str:\n",
    "    context = \"- \"+ \"\\n \".join(item['chunks'] for item in context_items)\n",
    "    \n",
    "    prompt = context\n",
    "    \n",
    "    return prompt\n",
    "query = 'what is the real identity of iron man in avengers'\n",
    "\n",
    "#Get relevant responses\n",
    "scores, indices = retrieve_relevant_resources(query = query, embeddings = embeddings)\n",
    "\n",
    "#Creat e alist of context items\n",
    "context_items = [scripts_with_chunks[i] for i in indices]\n",
    "\n",
    "#formatting our prompt\n",
    "prompt = prompt_formatter(query=instruction_str+query, context_items = context_items)\n",
    "print(len(prompt)//4)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-13T13:15:51.991672500Z",
     "start_time": "2024-04-13T13:15:51.990621900Z"
    }
   },
   "id": "36072c83c59e167b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "input_ids = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "\n",
    "outputs = model.get_generation(**input_ids,\n",
    "                               temperature = 0.7, #lower value makes the model more deterministic\n",
    "                               do_sample = True, #whether or not to use sampling https://huyenchip.com/2024/01/16/sampling.html\n",
    "                               \n",
    "                               )\n",
    "\n",
    "output_text= tokenizer.decode(outputs[0])\n",
    "print(f\"Query: {query}\")\n",
    "print(f\"RAG Answer:{output_text}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-13T13:15:51.991672500Z"
    }
   },
   "id": "4707eb05f213c7df"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
